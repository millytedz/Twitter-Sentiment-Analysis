{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554f2640",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "\n",
    "1.Case study : The much anticipated Netflix series, Wednesday was released on November 23 2022 and went on to break numerous                  records. A sentiment analysis was carried out to evaluate the perception among watchers around the globe.\n",
    "\n",
    "2.Data gathering : 75,000 tweets were scrapped from twitter using the python library snscraper between June 2022 when there was                    much anticipation till January 2023.\n",
    "\n",
    "3.Data wrangling : This done to fill the missing data and all neccessary adjustment to the raw scraped data\n",
    "\n",
    "4.Data Preprocessing : This involves this neccessay steps before carrying out the sentiment analysis to remove the stop words,                        tags, url links, Tokenizing the words Lemmitizing words with the use of various libraries in python\n",
    "\n",
    "5.Sentiment Analysis : A polarity score were first gotten with Textblob library which were then used to set up the conditions                          for the sentiments\n",
    "\n",
    "6.Data Visualization : Limited visualization can be done on python thus Power BI was opted for in visualizing and bringing                            insights to the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf411834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the necessary libraries needed\n",
    "\n",
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "\n",
    "from wordcloud import ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6276ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the query and condition for the scraped data\n",
    "\n",
    "tweets_list = []\n",
    "\n",
    "search = ('#WednesdayNetflix OR wednesday netflix OR Wednesday Netflix since:2022-06-01 until:2023-01-10 lang:en')\n",
    "\n",
    "\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(search).get_items()):\n",
    "    if i>75000:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.user.username, tweet.sourceLabel, tweet.id, tweet.rawContent, tweet.user.location, tweet.likeCount, tweet.retweetCount])\n",
    "\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime','Usernames', 'Source', 'Tweet Id', 'Tweet','Location', 'Number_of_likes', 'Number_of_retweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbae405",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('WednesdayAnalysis.csv', index=False)\n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ccd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.mask(tweets_df == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8594426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.Location.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae235d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.Location = tweets_df.Location.fillna('Unknown')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cce4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the casts from each tweets\n",
    "\n",
    "wednesday_casts= ['wednesday addams','jenna','morticia','valerie','donovan','hunter','tyler','percy',\n",
    "        'xavier','bianca','yoko','marilyn','eugene','thing','gomez','larissa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract casts from each Tweet\n",
    "def GetCasts(tweet):\n",
    "    tweet = tweet.lower() \n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    Casts = [char for char in tweet_tokens if char in wednesday_casts] \n",
    "    tweet = \" \".join(Casts)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2bab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['wednesday_casts'] = tweets_df['Tweet'].apply(GetCasts)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "stop_words = list(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0096e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji = list(UNICODE_EMOJI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef427a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreProcess the tweets to be ready for sentiment analysis\n",
    "def ProcessedTweets(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Cleaning and removing URLâ€™s\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags = re.MULTILINE)\n",
    "    \n",
    "    # Cleaning and removing repeating characters\n",
    "    tweet = re.sub(r'\\@\\w+|\\#\\w+|\\d+', '', tweet)\n",
    "    \n",
    "    # Cleaning and removing the above stop words list from the tweet text\n",
    "    tweet_tokens = word_tokenize(tweet)  \n",
    "    filtered_words = [w for w in tweet_tokens if w not in stop_words]\n",
    "    filtered_words = [w for w in filtered_words if w not in emoji]\n",
    "    \n",
    "    # Cleaning and removing punctuations\n",
    "    unpunctuated_words = [w for w in filtered_words if w not in string.punctuation]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in unpunctuated_words]\n",
    "    tweet = \" \".join(lemma_words)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a new colum for the processed tweets\n",
    "tweets_df['Processed Tweets'] = tweets_df['Tweet'].apply(ProcessedTweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16025f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e790ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tweet text into a string separated with \" \"\n",
    "tweets_string = tweets_df['Processed Tweets'].tolist()\n",
    "tweets_string = \" \".join(tweets_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7205155",
   "metadata": {},
   "source": [
    "Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ecf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First define the function to obtain Polarity score\n",
    "def Polarity(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "#Then set the condition for the polarity\n",
    "def SentimentTextBlob(polarity):\n",
    "    if polarity < 0:\n",
    "        return 'Negative'\n",
    "    elif polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the above functions to get polarity and sentiments\n",
    "\n",
    "tweets_df['Polarity']= tweets_df['Processed Tweets'].apply(Polarity)\n",
    "tweets_df['Sentiments']= tweets_df['Polarity'].apply(SentimentTextBlob)\n",
    "tweets_df.Sentiments.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the most talked about word in a word cloud \n",
    "# some stop words were still evident but was removed during visualization on Power BI\n",
    "# Instantiate the Twitter word cloud object\n",
    "\n",
    "word_cloud = WordCloud(collocations = False,max_words=200, background_color = 'black', width = 7000, height = 5000).generate(tweets_string)\n",
    "\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef383dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "thing_mask = np.array(Image.open('/Users/TEDZ/Downloads/mytopkid.com-wednesday-addams-clipart-7-600x844.png'))\n",
    "\n",
    "#Grab the mask colors\n",
    "colors = ImageColorGenerator(thing_mask)\n",
    "\n",
    "#Instantiate the wordcloud using color_func argument\n",
    "wordcloud = WordCloud(mask=thing_mask,\n",
    "                  background_color='white',\n",
    "                  color_func=colors).generate(tweets_string)\n",
    "\n",
    "#Plot the wordcloud\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('title')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87af29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data as a csv file \n",
    "\n",
    "tweets_df.to_csv('WednesdayNetflix.csv')\n",
    "word_cloud.to_file(\"wordcloud.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
